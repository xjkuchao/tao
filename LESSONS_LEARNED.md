# Lessons Learned

As a pure Rust multimedia codec framework fully driven and implemented by AI from scratch, this project also serves to document authentic experiences and critical evaluations regarding various AI programming tools and Large Language Models (LLMs) used throughout its development.

This file will be continuously updated as the project progresses.

### Evaluations of Tools and Models

_(To be added: reflections and insights will be recorded here dynamically as the development continues...)_

2026年02月20日 18:01:20 - Always execute the requested script immediately and report concrete outputs (record count, output file, and path) to avoid ambiguity.

2026年02月20日 18:04:15 - AI toolchains are most efficient in Linux. On Windows they often start with rg/ls and then use Unix pipes like | grep, fail, realize the shell is PowerShell, switch to commands like Get-Item, and can repeatedly degrade this way. In practice, developing on Linux (or using WSL on Windows) works much better and significantly reduces retries after tool-call errors.

2026年02月20日 18:40:09 - Current subscriptions: GitHub Pro (48 USD/year), Copilot Pro (100 USD/year), Cursor Pro (20 USD/month), Google AI Pro (750 THB/month), ChatGPT Pro (300 SGD/month), and Claude Enterprise (about 240 USD/month per Standard sub-account, billing details unclear, plus extra charges beyond Standard usage). Evaluation: Claude plugin/CLI/Desktop has the best product experience, with clear reasoning display and smooth performance even with very long histories; however, it feels the most expensive. In codec algorithm tuning, Standard can exhaust the 5-hour quota in about 3-4 rounds, and even an extra 15 USD pay-as-you-go top-up may not finish one complex tuning round. ChatGPT Pro is now the primary tool: expensive but strong for sustained complex codec debugging, with 5-hour usage usually staying below 20%. Cursor Pro appears very cost-effective in plan design, with separate billing for Auto mode and API, but after 4-5 days of intensive coding both can hit 100% and require waiting for monthly reset unless upgrading or enabling on-demand usage; since ChatGPT Pro is available, no further Cursor top-up was made. GitHub/Copilot Pro is mainly for VSCode, but VSCode also routes to GPT/Claude models, so actual usage is relatively low.

2026年02月20日 18:44:37 - Gemini 3.1 was just released today and is being tested with Antigravity. Antigravity provides an independent Agent Manager window, which is convenient because it can be opened and switched separately from the editor. However, the agent output process is mostly in English, and the reasoning display is less readable for Chinese users. In testing, Gemini 3.1 quickly helped finish the final AAC decoder tuning from 98% to 100% accuracy. It is still unclear whether this came from stronger model capability or from favorable chance in samples and tuning steps.

2026年02月20日 18:49:53 - When using GPT-5.2-Codex and Claude-4.6 for decoder development, the first implementation round usually failed badly against FFmpeg-exported reference audio samples, with initial accuracy often below 10%. It generally required many feedback-debug cycles by the AI itself, which felt somewhat luck-dependent. The first complex decoder was MPEG-4 Part 2; multiple models were rotated for about 2-3 days and accuracy still did not improve enough, which almost led to the conclusion that AI could not complete the task. This was also affected by the first-time planning workflow, which needed many iterations. After that, MP3 and Vorbis decoder work became much faster. Now a short instruction like 'use Vorbis as a template to create the XXX decoder development plan', plus two sample sets (1.xx and 2.xx), is often enough to let AI run forward. The takeaway is that AI performs better when repeatedly executing within a mature workflow, while creating a brand-new plan from scratch is hard to perfect in one pass.

2026年02月20日 18:56:16 - Claude repeatedly failed during Vorbis accuracy tuning with 'response too large' errors. Investigation showed the reasoning output was too complex and exceeded the maximum response size; setting export CLAUDE_CODE_MAX_OUTPUT_TOKENS=64000 resolved it. This also suggests a large portion of tokens may be consumed by reasoning output. At that stage, detailed reasoning visibility was not a priority, but the billing impact was real.
